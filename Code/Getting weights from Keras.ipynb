{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras\n",
    "\n",
    "This is some example code of how we can get the code in Keras, adapted from https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an example dataset from the Keras tutorial\n",
    "dataset = loadtxt('pima_indians.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "This is a bit of a funny one, as we define an optimizer in this part. In actuality, though, this is only because we want to use Keras to do loss evaluation, which requires doing _model.compile_. We need to pass an optimizer argument to this, but we actually never use it because we don't call _model.train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting stuff into the format we want\n",
    "\n",
    "Basically, we can simply set the weights of the neural net using the function _model.set_weights_. We can then evaluate the objective function for a subset of data using _model.evaluate(InputData, TrueOutput)_. Pseudocode for getting the objective function for a particular position would then look something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = some_input_data         #e.g. images\n",
    "y = some_true_outputs       #e.g. image labels\n",
    "\n",
    "def f(positions):\n",
    "    model.set_weights(positions)\n",
    "    return model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In actuality, the _get_weights_ function actually passes us a list of numpy arrays, so we have to do some manipulations, but this is pretty chill, and we don't really need to worry about it. Basically, what has happened is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Extract the information about the shape of the data we get from the _model.get_weights_ function (stored in ls_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_form = []\n",
    "my_weights = model.get_weights()\n",
    "index = 0\n",
    "for comp in my_weights:\n",
    "    ls_form.append([index, index + np.size(comp), comp.shape])\n",
    "    index += np.size(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Define a function for converting the format we get from keras into one long numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(pre_w):\n",
    "    position = []\n",
    "    for w in pre_w:\n",
    "        position.append(w.flatten())\n",
    "    return np.concatenate(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define a function for converting it back again. This uses the information we stored in the list ls_form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertBack(position):\n",
    "    global ls_form    \n",
    "    reinput = []\n",
    "    for i in ls_form:\n",
    "        reinput.append(position[i[0]:i[1]].reshape(i[2]))\n",
    "    return reinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Just to check that this works, let's take the weights, convert them into one long numpy array and back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221,)\n"
     ]
    }
   ],
   "source": [
    "b = Convert(x_0)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = ConvertBack(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ls_form)):\n",
    "    print(np.allclose(x_0[i], x_1[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We evaluate the loss associated with a model as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.67880415916443"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,y,verbose=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how long this takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.5 ms ± 550 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.evaluate(X,y,verbose=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the time it takes, on average, to calculate the loss associated with those weights for the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To clarify, neuron biases are included in the definition of \"weights\"\n",
    "* When calculating the loss, we are using the loss funtion defined in _model.compile_ (in this case, binary cross-entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
