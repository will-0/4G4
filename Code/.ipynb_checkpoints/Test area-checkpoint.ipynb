{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing area\n",
    "\n",
    "This is the area for us to test the performance of our swarm-optimized neural networks. The steps for testing a neural network are as follows:\n",
    "\n",
    "1. Define a model using the Keras framework\n",
    "\n",
    "2. Acquire some training and test data\n",
    "\n",
    "3. Provide both model and data to a swarm and begin optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [How to use the swarm optimizer](#howto)\n",
    "- [Testing on Iris dataset](#iris)\n",
    "- [Testing on MNIST dataset](#MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the swarm optimizer <a id=\"howto\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SwarmParty import NN_Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is create your own swarm, which can be created by making an instance of the swarm class. The is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_swarm = NN_Swarm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a swarm, with the parameters that can be defined at this point. These are *n\\_particles*, *x\\_min_*/*x\\_max*, *v_min*/*v_max*, *c_1* and *c_2*. By default, these parameters are chosen to have some values, but they can be be easily changed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_swarm.n_particles = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide a network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN_Swarms are specifically designed from optimization of keras-defined neural network models. To do this, let's first create a super simple keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='mse', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have made a neural net, you can pass it to the swarm using the *provide_model()* method, as demonstrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_swarm.provide_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The function described above also uses this model to infer the dimensionality of the data (from the number of weights and biases)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide data\n",
    "\n",
    "Now that it possesses a neural network model, the swarm works by using the model to evaluate the objective function for a given particles position at every iteration. The model needs to data to evaluate, so this is provided to the swarm next. Let's get some example data to pass to it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "enc = OneHotEncoder(); scaler = StandardScaler();\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']; y = iris['target']\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_swarm.provide_data(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using the swarm optimizer\n",
    "\n",
    "The PSO algorithm can then be run, using the parameters previously set, with the simple function call my_swarm.train():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_swarm.train(iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training curve\n",
    "\n",
    "You can do this using the inbuilt method \"plot_training\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_swarm.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can give the optional argument \"get_curve\" when you use my_swarm.train(), and this will give you back a numpy array containing a vector for the training data set, and a vector for the the validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Once you have defined a model, and your training/test data, you implement the swarm optimizer with the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_swarm2 = NN_Swarm()\n",
    "my_swarm2.provide_model(model)\n",
    "my_swarm2.provide_data(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_swarm2.train(iterations=10)\n",
    "my_swarm2.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the Iris data set\n",
    "\n",
    "Although the data set above was actually also the Iris one, we'll do a more thorough, and commented version here. This is copied straight out of McKrimble's notebook (PSO_NN_for_Iris)\n",
    "\n",
    "- [Definition of neural network and collection of dataset](#nniris)\n",
    "- [Experimental region](#irisexp)\n",
    "    * [Test 1: Default Parameters](#irisdefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of neural network and collection of dataset <a id=\"nniris\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "import pydot\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris_Xtrain shape = (120, 4)\n",
      "Iris_Ytrain shape = (120, 3)\n",
      "Iris_Xtest shape = (30, 4)\n",
      "Iris_Ytest shape = (30, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willh\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris['data'] # array of samples 4 dimensions each describing a feature\n",
    "y = iris['target'] # array of labels (0, 1, 2)\n",
    "names = iris['target_names'] # array of labels (0, 1, 2)\n",
    "feature_names = iris['feature_names'] # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray() # Y is output of 3 dimensions now, one hot encoding\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "Iris_Xtrain, Iris_Xtest, Iris_Ytrain, Iris_Ytest = train_test_split(X_scaled, Y, test_size=0.2, random_state=2)\n",
    "print(\"Iris_Xtrain shape = {}\".format(Iris_Xtrain.shape))\n",
    "print(\"Iris_Ytrain shape = {}\".format(Iris_Ytrain.shape))\n",
    "print(\"Iris_Xtest shape = {}\".format(Iris_Xtest.shape))\n",
    "print(\"Iris_Ytest shape = {}\".format(Iris_Ytest.shape))\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "irismodel = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "irismodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental region <a id=\"irisexp\"></a>\n",
    "\n",
    "This is where we can run our experiments and invesigate parameters etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1 - Default Parameters <a id=\"irisdefault\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 193)\n"
     ]
    }
   ],
   "source": [
    "iris_swarm1 = NN_Swarm()\n",
    "iris_swarm1.provide_model(irismodel)\n",
    "iris_swarm1.provide_data(Iris_Xtrain, Iris_Xtest, Iris_Ytrain, Iris_Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
