{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* [Neural network structure](#nn_structure)\n",
    "* [OOP version](#oop)\n",
    "* [PSO algorithm](#pso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our neural network structure <a id=\"nn_structure\"></a>\n",
    "\n",
    "Lifted from the tasty notebook of Krimbles McBarkerface. If tensorflow_docs is not installed, gotta use this bad boy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://www.github.com/tensorflow/docs; !pip3 install pydot; !pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "import pydot\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (120, 4)\n",
      "Y_train shape = (120, 3)\n",
      "X_test shape = (30, 4)\n",
      "Y_test shape = (30, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willh\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris['data'] # array of samples 4 dimensions each describing a feature\n",
    "y = iris['target'] # array of labels (0, 1, 2)\n",
    "names = iris['target_names'] # array of labels (0, 1, 2)\n",
    "feature_names = iris['feature_names'] # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray() # Y is output of 3 dimensions now, one hot encoding\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=2)\n",
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "print(\"Y_train shape = {}\".format(Y_train.shape))\n",
    "print(\"X_test shape = {}\".format(X_test.shape))\n",
    "print(\"Y_test shape = {}\".format(Y_test.shape))\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the method for extracting and converting weights to positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_form = []\n",
    "my_weights = model.get_weights()\n",
    "index = 0\n",
    "for comp in my_weights:\n",
    "    ls_form.append([index, index + np.size(comp), comp.shape])\n",
    "    index += np.size(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(pre_w):\n",
    "    position = []\n",
    "    for w in pre_w:\n",
    "        position.append(w.flatten())\n",
    "    return np.concatenate(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertBack(position):\n",
    "    global ls_form    \n",
    "    reinput = []\n",
    "    for i in ls_form:\n",
    "        reinput.append(position[i[0]:i[1]].reshape(i[2]))\n",
    "    return reinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train, Y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSO algorithm <a id=\"pso\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(positions, X_in = X_train, Y_in = Y_train):\n",
    "    if positions.ndim == 1:\n",
    "        positions = positions.reshape([1, positions.shape[0]])\n",
    "    global model, X_train, Y_train\n",
    "    objective = np.zeros([positions.shape[0], 1])\n",
    "    for part_ind, part_pos in enumerate(positions):\n",
    "        x_1 = ConvertBack(part_pos)\n",
    "        model.set_weights(x_1)\n",
    "        objective[part_ind] = model.evaluate(X_in, Y_in, verbose=0)\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters of the swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define swarm\n",
    "n_particles = 50\n",
    "n_dims = Convert(model.get_weights()).shape[0]\n",
    "\n",
    "#define parameters\n",
    "x_min = -1\n",
    "x_max = 1\n",
    "v_min = 0\n",
    "v_max = 0.1\n",
    "c_1 = 2\n",
    "c_2 = 2\n",
    "\n",
    "# Set the initial conditions\n",
    "current_pos = x_min + (x_max-x_min)*np.random.rand(n_particles, n_dims) #initialise the particles\n",
    "p_best = current_pos                                                    #set the particle best\n",
    "g_best = p_best[np.argmin(f(current_pos)),:];                           #get the best location\n",
    "\n",
    "v = v_min + (v_max-v_min)*np.random.rand(n_particles, n_dims)\n",
    "\n",
    "is_better = np.zeros([n_particles,1])\n",
    "\n",
    "pbest_perform = f(p_best)\n",
    "gbest_perform = f(g_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the animation function (swarm optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We REALLY need to alter this function to only evaluate the function once. This will speed up the algorithm by a factor of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations = 20):\n",
    "    global n_particles, n_dims, v, v_max, c_1, c_2, current_pos, p_best, pbest_perform, g_best, gbest_perform, is_better\n",
    "    training_curve = [];\n",
    "        \n",
    "    for i in range(iterations):\n",
    "        m_v_max = v_max#*g_best_perform\n",
    "\n",
    "        #update the positions using the velocity\n",
    "        v += c_1*(np.random.rand(current_pos.shape[0],1))*(p_best-current_pos)\n",
    "        v += c_2*(np.random.rand(current_pos.shape[0],1))*(g_best-current_pos)\n",
    "        v_norm = np.linalg.norm(v,axis=1).reshape([v.shape[0],1])             #code for velocity limitation\n",
    "        v = np.where(v_norm < m_v_max, v, m_v_max*v/v_norm)                       #(comment in to use it)\n",
    "        current_pos += v\n",
    "        curr_perform = f(current_pos)\n",
    "        #replace the p_bests with the current location if they're better\n",
    "        is_better = (curr_perform<pbest_perform).reshape([is_better.shape[0],1])\n",
    "        p_best = is_better*current_pos + np.logical_not(is_better)*p_best\n",
    "        pbest_perform = is_better*curr_perform + np.logical_not(is_better)*pbest_perform\n",
    "        #update g_best\n",
    "        if np.min(pbest_perform) < gbest_perform:\n",
    "            g_best = p_best[np.argmin(pbest_perform),:]\n",
    "            gbest_perform = np.min(pbest_perform)\n",
    "        print(\"\\rIteration \" + str(i+1) + \": \" + str(gbest_perform), end = \"\\r\")\n",
    "        training_curve.append([gbest_perform.astype(float), f(g_best, X_in = X_test, Y_in = Y_test)[0][0].astype(float)])\n",
    "    return np.array(training_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_batch(batch_size=30):\n",
    "    global n_particles, n_dims, v, v_max, c_1, c_2, current_pos, p_best, pbest_perform, g_best, gbest_perform, is_better\n",
    "    global X_train\n",
    "    for i in range(20):\n",
    "    #     m_v_max = v_max_initial*np.exp(i/20)\n",
    "        for ind in range((X_train.shape[0]//batch_size)+1):\n",
    "            X_batch = X_train[ind*batch_size:(ind+1)*batch_size]\n",
    "            Y_batch = Y_train[ind*batch_size:(ind+1)*batch_size]\n",
    "            #update the positions using the velocity\n",
    "            v += c_1*(np.random.rand(current_pos.shape[0],1))*(p_best-current_pos)\n",
    "            v += c_2*(np.random.rand(current_pos.shape[0],1))*(g_best-current_pos)\n",
    "            v_norm = np.linalg.norm(v,axis=1).reshape([v.shape[0],1])             #code for velocity limitation\n",
    "            v = np.where(v_norm < v_max, v, v_max*v/v_norm)                       #(comment in to use it)\n",
    "            current_pos += v\n",
    "            curr_perform = f(current_pos, X_in = X_batch, Y_in = Y_batch)\n",
    "            #replace the p_bests with the current location if they're better\n",
    "            is_better = (curr_perform<pbest_perform).reshape([is_better.shape[0],1])\n",
    "            p_best = is_better*current_pos + np.logical_not(is_better)*p_best\n",
    "            pbest_perform = is_better*curr_perform + np.logical_not(is_better)*pbest_perform\n",
    "            #update g_best\n",
    "            if np.min(pbest_perform) < gbest_perform:\n",
    "                g_best = p_best[np.argmin(pbest_perform),:]\n",
    "                gbest_perform = np.min(pbest_perform)\n",
    "            print(\"\\rIteration \" + str(i+1) + \".\" + str(ind) + \": \" + str(gbest_perform), end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train_data = train(iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m_train_data[:,0], label=\"Training\")\n",
    "plt.plot(m_train_data[:,1], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(g_best)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KrimbleWeights = np.load(\"SaveThoseWeights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f(KrimbleWeights)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = ConvertBack(g_best)\n",
    "model.set_weights(x_1)\n",
    "\n",
    "test_predictions = model.predict(X_test)\n",
    "NN_output_args = np.argmax(test_predictions, axis = 1)\n",
    "real_output_args = np.argmax(Y_test, axis = 1)\n",
    "\n",
    "plt.figure(figsize = (10,1))\n",
    "plt.plot(NN_output_args,'ro', label = \"NN\")\n",
    "plt.plot(real_output_args, 'bo', label = \"Real\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "test_accuracy(NN_output_args, real_output_args)\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the weights and biases for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"50_part_100_iters_tcurve\", m_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(positions):\n",
    "    if positions.ndim == 1:\n",
    "        positions = positions.reshape([1, positions.shape[0]])\n",
    "    global model, X_train, Y_train\n",
    "    objective = np.zeros([positions.shape[0], 1])\n",
    "    for part_ind, part_pos in enumerate(positions):\n",
    "        x_1 = ConvertBack(part_pos)\n",
    "        model.set_weights(x_1)\n",
    "        objective[part_ind] = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$$$\n",
    "$$$$\n",
    "$$$$\n",
    "$$$$\n",
    "$$$$\n",
    "$$$$\n",
    "$$$$\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-oriented version\n",
    "\n",
    "Have implemented an object-oriented version of this optimization algorithm, as the code is starting to look like spaghetti. You define a new swarm like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SwarmParty as swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm = swarm.NN_Swarm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm.provide_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm.provide_data(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance at iteration 50: 0.34793433149655664\r"
     ]
    }
   ],
   "source": [
    "training_curve = swarm.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
